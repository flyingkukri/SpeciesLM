{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad73d5d-d102-4d1b-83be-be648b67b2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import collections\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f11acc-8bfe-48a2-9c1f-c430ec52017a",
   "metadata": {},
   "source": [
    "# Basic Information\n",
    "\n",
    "## Sequence length constraints\n",
    "\n",
    "The 3' model expects an input sequence which is 300bp long (stop codon + 297). It will handle shorter sequences (although < 11 cannot be masked) and in theory can even predict up to 512 - but this is out-of-distribution and likely performs very poorly as the positional encodings are not adapted for this.\n",
    "\n",
    "The 5' model expects an input which is 1003bp long (1000 + start codon). Longer sequences will not work, shorter sequences must be padded (e.g. with a fixed sequence), otherwise the start codon gets the wrong positional encoding which confuses the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d75f9-86fd-401c-91a8-6668cb5a10a6",
   "metadata": {},
   "source": [
    "# Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aeba8-b342-4af1-bb1a-ea572d7b3f7a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2726f3f-96fd-4224-9430-0b8a3fd2151e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    # one-hot encodes the sequence\n",
    "    result = []\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    # make labels into ground truths\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    return np.array([nuc_dict[x] for x in gts])\n",
    "\n",
    "def tok_func_standard(x, seq_col): return tokenizer(\" \".join(kmers_stride1(x[seq_col])))\n",
    "\n",
    "def tok_func_species(x, species_proxy, seq_col):\n",
    "    res = tokenizer(species_proxy + \" \" +  \" \".join(kmers_stride1(x[seq_col])))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af032e3b-91e9-4d83-ba9d-e06ac7695235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_special_tokens(tokens, tokenizer, where = \"left\"):\n",
    "    count = 0\n",
    "    if where == \"right\":\n",
    "        tokens = tokens[::-1]\n",
    "    for pos in range(len(tokens)):\n",
    "        tok = tokens[pos]\n",
    "        if tok in tokenizer.all_special_ids:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cabdc6-b722-4951-86b0-34484c5e28f7",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0633d55f-d473-4408-bfd6-e783be64027b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_df_path = \"data/Sequences/Annotation/Sequences/saccharomyces_cerevisiae/saccharomyces_cerevisiae_three_prime.parquet\"\n",
    "\n",
    "seq_col = \"three_prime_seq\" # name of the column in the df that stores the sequences\n",
    "kmer_size = 6 # size of kmers, always 6\n",
    "proxy_species = \"candida_glabrata\" # species token to use\n",
    "pred_batch_size = 128*3 # batch size for rolling masking\n",
    "target_layer = (8,) # what hidden layers to use for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28cb53-c260-47f0-96b8-183f3b04e271",
   "metadata": {},
   "source": [
    "# Load Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215fe89-40e0-4cdd-a2c1-c878a5f38890",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ef43d6-01fc-4a1f-8b06-410bd37ff066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9681f868-65d1-4d30-995b-83c8d5503c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "#model.to(torch.bfloat16).to(device)\n",
    "#model.to(torch.float16).to(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8126f0-c09b-4a07-a2d1-e9c6cc83fba7",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3616aa-6335-417d-a9ff-154a2f4c3d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(seq_df_path)\n",
    "dataset[seq_col] = dataset[seq_col].str[:300] # truncate longer sequences\n",
    "dataset = dataset.loc[dataset[seq_col].str.len() == 300] # throw out too short sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63822cc7-5c93-45db-bc65-9ef4fc67e224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/6594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_func = lambda x: tok_func_species(x, proxy_species, seq_col)\n",
    "\n",
    "ds = Dataset.from_pandas(dataset[[seq_col]])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns(seq_col)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ae42c-e2ff-406e-956f-6dc2124d631f",
   "metadata": {},
   "source": [
    "# Reconstruction Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd306845-4a1d-435b-b5f6-1fd63121e1ce",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b78f821-b12f-4cb1-b931-93b381b316b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_batch_generator(tokenized_data, dataset, seq_idx, \n",
    "                               special_token_offset, \n",
    "                               kmer_size = kmer_size,\n",
    "                               seq_col = seq_col,\n",
    "                               pred_batch_size = pred_batch_size):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx][seq_col]\n",
    "    label_len = len(label)\n",
    "    if label_len < kmer_size:\n",
    "        print(\"This should not occur\")\n",
    "        return torch.zeros(label_len,label_len,5)\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[2+special_token_offset:label_len-(kmer_size-1)-3+special_token_offset]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        res[masked_indices] = 4\n",
    "        yield res.shape[0] # provide the total size\n",
    "        for batch_idx in range(math.ceil(res.shape[0]/pred_batch_size)):\n",
    "            res_batch = res[batch_idx*pred_batch_size:(batch_idx+1)*pred_batch_size]\n",
    "            res_batch = res_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                computation = model(res_batch)\n",
    "                logits = computation[\"logits\"].detach()\n",
    "                #if \"logits\" in computation:\n",
    "                #    logits = computation[\"logits\"].detach()\n",
    "                #else:\n",
    "                #    logits = computation[\"prediction_logits\"].float().detach()\n",
    "                fin_calculation = logits\n",
    "            yield fin_calculation, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b32b2b5-60b4-43cc-bb01-9ee4cf9ae31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a convolutional filter for each nt\n",
    "# the way this works:\n",
    "# The kmer ACGTGC\n",
    "# maps to token 739\n",
    "# the last nt is C\n",
    "# this would be the prediction for the masked nucleotide\n",
    "# from this kmer, if the kmer is the first in masked span\n",
    "# so the first row of column 739 searches for C\n",
    "# in other words filter_ijk = 1 for i = 0, j = 739, k = 2\n",
    "vocab = tokenizer.get_vocab()\n",
    "kmer_list = [\"\".join(x) for x in itertools.product(\"ACGT\",repeat=6)]\n",
    "nt_mapping = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "prb_filter = np.zeros((kmer_size, 4**kmer_size, 4))\n",
    "for kmer in kmer_list:\n",
    "    token = vocab[kmer] - 5 # there are 5 special tokens\n",
    "    for idx, nt in enumerate(kmer):\n",
    "        nt_idx = nt_mapping[nt]\n",
    "        prb_filter[5-idx, token, nt_idx] = 1\n",
    "prb_filter = torch.from_numpy(prb_filter)\n",
    "prb_filter = prb_filter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd36a5a8-660e-4b22-8fae-561d8301c8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(kmer_prediction, \n",
    "                           label_pos,\n",
    "                           max_pos,\n",
    "                           prb_filter=prb_filter,\n",
    "                           kmer_size=kmer_size):   \n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    nt_preds = kmer_prediction[label_pos:(label_pos+kmer_size),:] # extract the right kmers\n",
    "    nt_preds = nt_preds.unsqueeze(2).expand((nt_preds.shape[0],nt_preds.shape[1],4)) # repeat along nt dimension\n",
    "    nt_preds = (nt_preds*prb_filter).sum(axis=1) # filter and add over tokens\n",
    "    nt_preds = nt_preds.sum(axis=0)\n",
    "    nt_prbs = nt_preds/nt_preds.sum() # renormalize\n",
    "    return nt_prbs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb38aa-bae4-48c0-8d30-d60a2d0e8a72",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0e917-d309-49dc-936f-e95b90aa3ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_prbs,gts = [],[]\n",
    "prev_len = 0\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    #if no_of_index > 10:\n",
    "    #    break\n",
    "    label = dataset.iloc[no_of_index][seq_col]\n",
    "    label_len = len(label)\n",
    "    \n",
    "    left_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"left\")\n",
    "    right_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"right\")\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 11: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25]))\n",
    "            gts.append(label[i])\n",
    "        added_len = len(predicted_prbs) - prev_len\n",
    "        prev_len = len(predicted_prbs)\n",
    "        assert added_len == len(label)\n",
    "        continue\n",
    "\n",
    "    # we do a batched predict to process the sequence\n",
    "    batch_start = 0\n",
    "    pos = 0\n",
    "    prediction_generator = predict_on_batch_generator(tokenized_data, dataset, no_of_index, special_token_offset = left_special_tokens)\n",
    "    max_idx = next(prediction_generator)\n",
    "    for predictions, res in prediction_generator:\n",
    "    \n",
    "        # prepare predictions for processing\n",
    "        logits = predictions[:,:,5:(5+prb_filter.shape[1])] # remove any non k-mer dims\n",
    "        kmer_preds = torch.softmax(logits,dim=2)\n",
    "        # remove special tokens:\n",
    "        kmer_preds = kmer_preds[:,(left_special_tokens):(kmer_preds.shape[1] - right_special_tokens),:]\n",
    "        max_pos = kmer_preds.shape[1] - 1\n",
    "        # pad to predict first 5 and last 5 nt\n",
    "        padded_tensor = torch.zeros((kmer_preds.shape[0],2*(kmer_size-1) + kmer_preds.shape[1],kmer_preds.shape[2]),device=device)\n",
    "        padded_tensor[:,kmer_size-1:-(kmer_size-1),:] = kmer_preds\n",
    "        kmer_preds = padded_tensor\n",
    "        \n",
    "        while pos < label_len:\n",
    "            # get prediction\n",
    "            theoretical_idx = min(max(pos-5,0),max_idx-1) # idx if we did it all in one batch\n",
    "            actual_idx = max(theoretical_idx - batch_start,0) \n",
    "            if actual_idx >= kmer_preds.shape[0]:\n",
    "                break\n",
    "            kmer_prediction = kmer_preds[actual_idx]\n",
    "            nt_prbs = extract_prbs_from_pred(kmer_prediction=kmer_prediction, \n",
    "                                             label_pos=pos,\n",
    "                                             max_pos=max_pos)\n",
    "            predicted_prbs.append(nt_prbs)\n",
    "            # extract ground truth\n",
    "            gt = label[pos]\n",
    "            gts.append(gt)\n",
    "            # update\n",
    "            pos += 1\n",
    "        \n",
    "        batch_start = pos - 5\n",
    "\n",
    "    added_len = len(predicted_prbs) - prev_len\n",
    "    prev_len = len(predicted_prbs)\n",
    "    assert added_len == len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d274b04-7438-4a79-830a-426f9cfa8b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prbs_arr = np.stack(predicted_prbs).reshape((no_of_index, 300, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788438d-ce40-476f-8bdc-ed77bad0bfe5",
   "metadata": {},
   "source": [
    "# Embedding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf37faa-0ddf-4cbb-9507-19005b96dc84",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_on_batch(tokenized_data, dataset, seq_idx, \n",
    "                   special_token_offset,\n",
    "                   target_layer = target_layer):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx][seq_col]\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        print(\"This should not occur\")\n",
    "        return torch.zeros(label_len,label_len,768)\n",
    "    else:\n",
    "        res = tokenized_data['input_ids'].clone()\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(res, output_hidden_states=True)['hidden_states'] \n",
    "    if isinstance(target_layer, int):    \n",
    "        embedding = embedding[target_layer]\n",
    "    elif len(target_layer) == 1:\n",
    "        embedding = torch.stack(embedding[target_layer[0]:],axis=0)\n",
    "        embedding = torch.mean(embedding, axis=0)\n",
    "    else:\n",
    "        embedding = torch.stack(embedding[target_layer[0]:target_layer[1]],axis=0)\n",
    "        embedding = torch.mean(embedding, axis=0)   \n",
    "    embedding = embedding.detach().cpu().numpy() \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ceb58cb-8cfb-4de8-bd6a-ee5e73537952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embedding_from_pred(hidden_states, batch_pos):   \n",
    "    pred_pos_min = min(max(pos - 5, 0), hidden_states.shape[1]-1)\n",
    "    pred_pos_max = min(max(pos, 0), hidden_states.shape[1]-1)\n",
    "    token_embedding = hidden_states[batch_pos, pred_pos_min:pred_pos_max+1, :]\n",
    "    token_embedding = token_embedding.mean(axis=0)\n",
    "    return token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652363-06c5-4c75-9ca2-d480da793fec",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26683cd0-c57c-4c70-b3a1-7e2d64e0e474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6594it [01:07, 97.36it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "averaged_embeddings = []\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    embeddings = []\n",
    "\n",
    "    label = dataset.iloc[no_of_index][seq_col]\n",
    "    label_len = len(label)\n",
    "    \n",
    "    left_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"left\")\n",
    "    right_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"right\")\n",
    "\n",
    "    if label_len < 11: \n",
    "        averaged_embeddings.append(np.array([0.0]*768))\n",
    "        continue\n",
    "\n",
    "    hidden_states = embed_on_batch(tokenized_data, dataset, no_of_index, special_token_offset = left_special_tokens)\n",
    "    avg = hidden_states.mean(axis=(0,1))\n",
    "    \n",
    "    averaged_embeddings.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbbc000c-e1f2-4bdf-829f-78b92cc95edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = np.stack(averaged_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-pytorch_enformer]",
   "language": "python",
   "name": "conda-env-anaconda-pytorch_enformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
