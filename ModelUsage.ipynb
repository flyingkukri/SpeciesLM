{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad73d5d-d102-4d1b-83be-be648b67b2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import collections\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f11acc-8bfe-48a2-9c1f-c430ec52017a",
   "metadata": {},
   "source": [
    "# Basic Information\n",
    "\n",
    "## Sequence length constraints\n",
    "\n",
    "The 3' model expects an input sequence which is 300bp long (stop codon + 297). It will handle shorter sequences (although < 11 cannot be masked) and in theory can even predict up to 512 - but this is out-of-distribution and likely performs very poorly as the positional encodings are not adapted for this.\n",
    "\n",
    "The 5' model expects an input which is 1003bp long (1000 + start codon). Longer sequences will not work, shorter sequences must be padded (e.g. with a fixed sequence), otherwise the start codon gets the wrong positional encoding which confuses the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d75f9-86fd-401c-91a8-6668cb5a10a6",
   "metadata": {},
   "source": [
    "# Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aeba8-b342-4af1-bb1a-ea572d7b3f7a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2726f3f-96fd-4224-9430-0b8a3fd2151e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    # one-hot encodes the sequence\n",
    "    result = []\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    # make labels into ground truths\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    return np.array([nuc_dict[x] for x in gts])\n",
    "\n",
    "def tok_func_standard(x, seq_col): return tokenizer(\" \".join(kmers_stride1(x[seq_col])))\n",
    "\n",
    "def tok_func_species(x, species_proxy, seq_col):\n",
    "    res = tokenizer(species_proxy + \" \" +  \" \".join(kmers_stride1(x[seq_col])))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af032e3b-91e9-4d83-ba9d-e06ac7695235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_special_tokens(tokens, tokenizer, where = \"left\"):\n",
    "    count = 0\n",
    "    if where == \"right\":\n",
    "        tokens = tokens[::-1]\n",
    "    for pos in range(len(tokens)):\n",
    "        tok = tokens[pos]\n",
    "        if tok in tokenizer.all_special_ids:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66621b2-7bd0-4a1c-afb1-853437be5362",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67c1345-dc03-438a-b48c-4091635565a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import  DataCollatorForLanguageModeling\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability = 0.15)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "class DataCollatorForLanguageModelingSpan():\n",
    "    \n",
    "    def __init__(self, tokenizer, mlm, mlm_probability, span_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.span_length = span_length\n",
    "        self.mlm_probability= mlm_probability\n",
    "        self.pad_to_multiple_of = span_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask):\n",
    "        import torch\n",
    "        \n",
    "        original_inputs = inputs.clone()\n",
    "        labels = inputs.clone()\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        # 10% of the time, we leave the input unchanged\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.1)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        # to ensure that we create spans, we convolve with a filter length of 6\n",
    "        # we convert back to bool to account for overlaps (anything bigger than 0 gets masked)\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_save = masked_indices.clone()\n",
    "        \n",
    "        # 10% of the time, we replace masked input tokens with random nt\n",
    "        # create a random offset matrix (randint(1,4))\n",
    "        offsets = torch.randint(1, 4, labels.shape)\n",
    "        # multiply with a masking matrix to get masked offsets\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.1)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        probability_matrix.masked_fill_(m_save, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        masked_offsets = masked_indices*offsets\n",
    "        # make the modulo matrix which identifies the nucleotide\n",
    "        modulo_matrix = torch.remainder(inputs - 5, 4) # adjust for special tokens\n",
    "        # Get the masked shifts matrix\n",
    "        masked_shifts = torch.remainder(modulo_matrix + masked_offsets, 4) - modulo_matrix\n",
    "        # we now propagate the change to the next few tokens\n",
    "        for i in range(self.span_length):\n",
    "            shifted_shift = masked_shifts[:,:(masked_shifts.shape[1] - i)]\n",
    "            inputs[:,i:] = (shifted_shift*(4**i)) + inputs[:,i:]\n",
    "            masked_indices[:,i:] = shifted_shift + masked_indices[:,i:]\n",
    "        masked_indices = masked_indices.bool()\n",
    "        m_save = m_save + masked_indices\n",
    "        \n",
    "        # 80% of the time, we mask\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.8) \n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        probability_matrix.masked_fill_(m_save, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        # to ensure that we create spans, we convolve with a filter length of 6\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        \n",
    "        # aggregate all the positions where we want loss\n",
    "        m_final = masked_indices + m_save \n",
    "        labels[~m_final] = -100  # We only compute loss on masked tokens\n",
    "        # we actually replace with the mask token\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        \n",
    "        # restore the special tokens\n",
    "        inputs[special_tokens_mask] = original_inputs[special_tokens_mask]\n",
    "        labels[special_tokens_mask] = -100\n",
    "        \n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cabdc6-b722-4951-86b0-34484c5e28f7",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0633d55f-d473-4408-bfd6-e783be64027b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_df_path = \"data/Sequences/Annotation/Sequences/saccharomyces_cerevisiae/saccharomyces_cerevisiae_three_prime.parquet\"\n",
    "\n",
    "seq_col = \"three_prime_seq\" # name of the column in the df that stores the sequences\n",
    "kmer_size = 6 # size of kmers, always 6\n",
    "proxy_species = \"candida_glabrata\" # species token to use\n",
    "pred_batch_size = 128*3 # batch size for rolling masking\n",
    "target_layer = (8,) # what hidden layers to use for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28cb53-c260-47f0-96b8-183f3b04e271",
   "metadata": {},
   "source": [
    "# Load Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215fe89-40e0-4cdd-a2c1-c878a5f38890",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ef43d6-01fc-4a1f-8b06-410bd37ff066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9a6657393f4aaea06e41fbe935a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cb0b19fdbe4b3eae33302b4b02161b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)species_lm/vocab.txt:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aee7a0d044b4ce79c3a9ae9f9b2f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)es_lm/tokenizer.json:   0%|          | 0.00/379k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085a00bc5d1b486d96f1b4c1efdade84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)lm/added_tokens.json:   0%|          | 0.00/68.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8d4fbb449548808c7d71b286d3dd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/62.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d059d1c325436aab7104cf5f102175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)ecies_lm/config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcce28ffd7b453c9a770a27fec20d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/361M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9681f868-65d1-4d30-995b-83c8d5503c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "#model.to(torch.bfloat16).to(device)\n",
    "#model.to(torch.float16).to(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8126f0-c09b-4a07-a2d1-e9c6cc83fba7",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e3616aa-6335-417d-a9ff-154a2f4c3d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(seq_df_path)\n",
    "dataset[seq_col] = dataset[seq_col].str[:300] # truncate longer sequences\n",
    "dataset = dataset.loc[dataset[seq_col].str.len() == 300] # throw out too short sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63822cc7-5c93-45db-bc65-9ef4fc67e224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/6594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_func = lambda x: tok_func_species(x, proxy_species, seq_col)\n",
    "\n",
    "ds = Dataset.from_pandas(dataset[[seq_col]])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns(seq_col)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingSpan(tokenizer, mlm=False, mlm_probability = 0, span_length = 6)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ae42c-e2ff-406e-956f-6dc2124d631f",
   "metadata": {},
   "source": [
    "# Reconstruction Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd306845-4a1d-435b-b5f6-1fd63121e1ce",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b78f821-b12f-4cb1-b931-93b381b316b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_batch_generator(tokenized_data, dataset, seq_idx, \n",
    "                               special_token_offset, \n",
    "                               kmer_size = kmer_size,\n",
    "                               seq_col = seq_col,\n",
    "                               pred_batch_size = pred_batch_size):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx][seq_col]\n",
    "    label_len = len(label)\n",
    "    if label_len < kmer_size:\n",
    "        print(\"This should not occur\")\n",
    "        return torch.zeros(label_len,label_len,5)\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[2+special_token_offset:label_len-(kmer_size-1)-3+special_token_offset]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        res[masked_indices] = 4\n",
    "        yield res.shape[0] # provide the total size\n",
    "        for batch_idx in range(math.ceil(res.shape[0]/pred_batch_size)):\n",
    "            res_batch = res[batch_idx*pred_batch_size:(batch_idx+1)*pred_batch_size]\n",
    "            res_batch = res_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                computation = model(res_batch)\n",
    "                logits = computation[\"logits\"].detach()\n",
    "                #if \"logits\" in computation:\n",
    "                #    logits = computation[\"logits\"].detach()\n",
    "                #else:\n",
    "                #    logits = computation[\"prediction_logits\"].float().detach()\n",
    "                fin_calculation = logits\n",
    "            yield fin_calculation, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b32b2b5-60b4-43cc-bb01-9ee4cf9ae31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a convolutional filter for each nt\n",
    "# the way this works:\n",
    "# The kmer ACGTGC\n",
    "# maps to token 739\n",
    "# the last nt is C\n",
    "# this would be the prediction for the masked nucleotide\n",
    "# from this kmer, if the kmer is the first in masked span\n",
    "# so the first row of column 739 searches for C\n",
    "# in other words filter_ijk = 1 for i = 0, j = 739, k = 2\n",
    "vocab = tokenizer.get_vocab()\n",
    "kmer_list = [\"\".join(x) for x in itertools.product(\"ACGT\",repeat=6)]\n",
    "nt_mapping = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "prb_filter = np.zeros((kmer_size, 4**kmer_size, 4))\n",
    "for kmer in kmer_list:\n",
    "    token = vocab[kmer] - 5 # there are 5 special tokens\n",
    "    for idx, nt in enumerate(kmer):\n",
    "        nt_idx = nt_mapping[nt]\n",
    "        prb_filter[5-idx, token, nt_idx] = 1\n",
    "prb_filter = torch.from_numpy(prb_filter)\n",
    "prb_filter = prb_filter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd36a5a8-660e-4b22-8fae-561d8301c8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(kmer_prediction, \n",
    "                           label_pos,\n",
    "                           max_pos,\n",
    "                           prb_filter=prb_filter,\n",
    "                           kmer_size=kmer_size):   \n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    nt_preds = kmer_prediction[label_pos:(label_pos+kmer_size),:] # extract the right kmers\n",
    "    nt_preds = nt_preds.unsqueeze(2).expand((nt_preds.shape[0],nt_preds.shape[1],4)) # repeat along nt dimension\n",
    "    nt_preds = (nt_preds*prb_filter).sum(axis=1) # filter and add over tokens\n",
    "    nt_preds = nt_preds.sum(axis=0)\n",
    "    nt_prbs = nt_preds/nt_preds.sum() # renormalize\n",
    "    return nt_prbs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb38aa-bae4-48c0-8d30-d60a2d0e8a72",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a0e917-d309-49dc-936f-e95b90aa3ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:14,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_prbs,gts = [],[]\n",
    "prev_len = 0\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    #if no_of_index > 10:\n",
    "    #    break\n",
    "    label = dataset.iloc[no_of_index][seq_col]\n",
    "    label_len = len(label)\n",
    "    \n",
    "    left_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"left\")\n",
    "    right_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"right\")\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 11: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25]))\n",
    "            gts.append(label[i])\n",
    "        added_len = len(predicted_prbs) - prev_len\n",
    "        prev_len = len(predicted_prbs)\n",
    "        assert added_len == len(label)\n",
    "        continue\n",
    "\n",
    "    # we do a batched predict to process the sequence\n",
    "    batch_start = 0\n",
    "    pos = 0\n",
    "    prediction_generator = predict_on_batch_generator(tokenized_data, dataset, no_of_index, special_token_offset = left_special_tokens)\n",
    "    max_idx = next(prediction_generator)\n",
    "    for predictions, res in prediction_generator:\n",
    "    \n",
    "        # prepare predictions for processing\n",
    "        logits = predictions[:,:,5:(5+prb_filter.shape[1])] # remove any non k-mer dims\n",
    "        kmer_preds = torch.softmax(logits,dim=2)\n",
    "        # remove special tokens:\n",
    "        kmer_preds = kmer_preds[:,(left_special_tokens):(kmer_preds.shape[1] - right_special_tokens),:]\n",
    "        max_pos = kmer_preds.shape[1] - 1\n",
    "        # pad to predict first 5 and last 5 nt\n",
    "        padded_tensor = torch.zeros((kmer_preds.shape[0],2*(kmer_size-1) + kmer_preds.shape[1],kmer_preds.shape[2]),device=device)\n",
    "        padded_tensor[:,kmer_size-1:-(kmer_size-1),:] = kmer_preds\n",
    "        kmer_preds = padded_tensor\n",
    "        \n",
    "        while pos < label_len:\n",
    "            # get prediction\n",
    "            theoretical_idx = min(max(pos-5,0),max_idx-1) # idx if we did it all in one batch\n",
    "            actual_idx = max(theoretical_idx - batch_start,0) \n",
    "            if actual_idx >= kmer_preds.shape[0]:\n",
    "                break\n",
    "            kmer_prediction = kmer_preds[actual_idx]\n",
    "            nt_prbs = extract_prbs_from_pred(kmer_prediction=kmer_prediction, \n",
    "                                             label_pos=pos,\n",
    "                                             max_pos=max_pos)\n",
    "            predicted_prbs.append(nt_prbs)\n",
    "            # extract ground truth\n",
    "            gt = label[pos]\n",
    "            gts.append(gt)\n",
    "            # update\n",
    "            pos += 1\n",
    "        \n",
    "        batch_start = pos - 5\n",
    "\n",
    "    added_len = len(predicted_prbs) - prev_len\n",
    "    prev_len = len(predicted_prbs)\n",
    "    assert added_len == len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d274b04-7438-4a79-830a-426f9cfa8b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prbs_arr = np.stack(predicted_prbs).reshape((no_of_index, 300, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d26b2fd-b99a-496f-ba33-1a0245d9a731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.3996e-05, 3.1054e-05, 2.2263e-04, 9.9969e-01],\n",
       "        [6.7587e-01, 9.7943e-05, 3.2381e-01, 2.2240e-04],\n",
       "        [7.6734e-01, 1.1883e-04, 2.3232e-01, 2.1588e-04],\n",
       "        ...,\n",
       "        [3.0795e-01, 2.0922e-01, 1.9542e-01, 2.8741e-01],\n",
       "        [2.9296e-01, 1.2911e-01, 1.8747e-01, 3.9046e-01],\n",
       "        [2.2976e-01, 2.7151e-01, 1.7726e-01, 3.2147e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"outputs/bertadn_origtest_convolved_prb_candida_glabrata_scer_downstream_fixedlen_512_withstop/prbs.pt\").reshape((len(dataset),300,4))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788438d-ce40-476f-8bdc-ed77bad0bfe5",
   "metadata": {},
   "source": [
    "# Embedding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf37faa-0ddf-4cbb-9507-19005b96dc84",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_on_batch(tokenized_data, dataset, seq_idx, \n",
    "                   special_token_offset,\n",
    "                   target_layer = target_layer):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx][seq_col]\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        print(\"This should not occur\")\n",
    "        return torch.zeros(label_len,label_len,768)\n",
    "    else:\n",
    "        res = tokenized_data['input_ids'].clone()\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(res, output_hidden_states=True)['hidden_states'] \n",
    "    if isinstance(target_layer, int):    \n",
    "        embedding = embedding[target_layer]\n",
    "    elif len(target_layer) == 1:\n",
    "        embedding = torch.stack(embedding[target_layer[0]:],axis=0)\n",
    "        embedding = torch.mean(embedding, axis=0)\n",
    "    else:\n",
    "        embedding = torch.stack(embedding[target_layer[0]:target_layer[1]],axis=0)\n",
    "        embedding = torch.mean(embedding, axis=0)   \n",
    "    embedding = embedding.detach().cpu().numpy() \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ceb58cb-8cfb-4de8-bd6a-ee5e73537952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embedding_from_pred(hidden_states, batch_pos):   \n",
    "    pred_pos_min = min(max(pos - 5, 0), hidden_states.shape[1]-1)\n",
    "    pred_pos_max = min(max(pos, 0), hidden_states.shape[1]-1)\n",
    "    token_embedding = hidden_states[batch_pos, pred_pos_min:pred_pos_max+1, :]\n",
    "    token_embedding = token_embedding.mean(axis=0)\n",
    "    return token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652363-06c5-4c75-9ca2-d480da793fec",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26683cd0-c57c-4c70-b3a1-7e2d64e0e474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "6594it [01:18, 83.97it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "averaged_embeddings = []\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    embeddings = []\n",
    "\n",
    "    label = dataset.iloc[no_of_index][seq_col]\n",
    "    label_len = len(label)\n",
    "    \n",
    "    left_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"left\")\n",
    "    right_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"right\")\n",
    "\n",
    "    if label_len < 11: \n",
    "        averaged_embeddings.append(np.array([0.0]*768))\n",
    "        continue\n",
    "\n",
    "    hidden_states = embed_on_batch(tokenized_data, dataset, no_of_index, special_token_offset = left_special_tokens)\n",
    "    avg = hidden_states.mean(axis=(0,1))\n",
    "    \n",
    "    averaged_embeddings.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbbc000c-e1f2-4bdf-829f-78b92cc95edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = np.stack(averaged_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-pytorch_enformer]",
   "language": "python",
   "name": "conda-env-anaconda-pytorch_enformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
