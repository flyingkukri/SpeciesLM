{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad73d5d-d102-4d1b-83be-be648b67b2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import collections\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6f913-9af8-4fdf-8061-12b1c4634bb2",
   "metadata": {},
   "source": [
    "## So that we can use the correct data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d335f67a-3517-43d8-90bf-0fc343bd535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../dnam2/bert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0b5075-273c-4fcf-a03f-f9cb6a51d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collate import DataCollatorForLanguageModelingSpan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f11acc-8bfe-48a2-9c1f-c430ec52017a",
   "metadata": {},
   "source": [
    "# Basic Information\n",
    "\n",
    "## Sequence length constraints\n",
    "\n",
    "The 3' model expects an input sequence which is 300bp long (stop codon + 297). It will handle shorter sequences (although < 11 cannot be masked) and in theory can even predict up to 512 - but this is out-of-distribution and likely performs very poorly as the positional encodings are not adapted for this.\n",
    "\n",
    "The 5' model expects an input which is 1003bp long (1000 + start codon). Longer sequences will not work, shorter sequences must be padded (e.g. with a fixed sequence), otherwise the start codon gets the wrong positional encoding which confuses the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d75f9-86fd-401c-91a8-6668cb5a10a6",
   "metadata": {},
   "source": [
    "# Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aeba8-b342-4af1-bb1a-ea572d7b3f7a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2726f3f-96fd-4224-9430-0b8a3fd2151e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    # one-hot encodes the sequence\n",
    "    result = []\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    # make labels into ground truths\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    return np.array([nuc_dict[x] for x in gts])\n",
    "\n",
    "def tok_func_standard(x, seq_col): return tokenizer(\" \".join(kmers_stride1(x[seq_col])))\n",
    "\n",
    "def tok_func_species(x, species_proxy, seq_col):\n",
    "    res = tokenizer(species_proxy + \" \" +  \" \".join(kmers_stride1(x[seq_col])))\n",
    "    return res\n",
    "\n",
    "def tok_func_embed(x, species_proxy, seq_col):\n",
    "    start_species_id = tokenizer.convert_tokens_to_ids([\"GGGGGG\"])[0]\n",
    "    res = tokenizer(\" \".join(kmers_stride1(x[seq_col])))\n",
    "    species_id = tokenizer.convert_tokens_to_ids(species_proxy)\n",
    "    species_id = species_id - start_species_id\n",
    "    res[\"species_id\"] = np.array([species_id])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af032e3b-91e9-4d83-ba9d-e06ac7695235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_special_tokens(tokens, tokenizer, where = \"left\"):\n",
    "    count = 0\n",
    "    if where == \"right\":\n",
    "        tokens = tokens[::-1]\n",
    "    for pos in range(len(tokens)):\n",
    "        tok = tokens[pos]\n",
    "        if tok in tokenizer.all_special_ids:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cabdc6-b722-4951-86b0-34484c5e28f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0633d55f-d473-4408-bfd6-e783be64027b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO change path\n",
    "scer_path = \"/s/project/semi_supervised_multispecies/Downstream/Sequences/Annotation/Sequences/saccharomyces_cerevisiae/saccharomyces_cerevisiae_three_prime.parquet\"\n",
    "\n",
    "# Downstream\n",
    "scer_ds_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/data/Sequences/Annotation/Sequences/saccharomyces_cerevisiae/saccharomyces_cerevisiae_three_prime.parquet\"\n",
    "pombe_ds_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/data/Sequences/Annotation/Sequences/schizosaccharomyces_pombe/schizosaccharomyces_pombe_three_prime_remapped.tsv\"\n",
    "pombe_pq_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/data/Sequences/Annotation/Sequences/schizosaccharomyces_pombe/schizosaccharomyces_pombe_three_prime.parquet\"\n",
    "three_sequences = \"/s/project/semi_supervised_multispecies/Downstream/Sequences/Annotation/Sequences/schizosaccharomyces_pombe/schizosaccharomyces_pombe_three_prime_remapped.tsv\"\n",
    "\n",
    "mpra_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/data/Downstream_Targets/segal_2015.tsv\"\n",
    "\n",
    "seq_df_path = scer_path\n",
    "\n",
    "seq_col = \"three_prime_seq\" # name of the column in the df that stores the sequences\n",
    "# seq_col = \"three_prime_region\" # for s. pombe\n",
    "\n",
    "kmer_size = 6 # size of kmers, always 6\n",
    "proxy_species = \"candida_glabrata\" # species token to use\n",
    "# proxy_species = \"schizosaccharomyces_pombe\"\n",
    "pred_batch_size = 128*3 # batch size for rolling masking\n",
    "target_layer = (8,) # what hidden layers to use for embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28cb53-c260-47f0-96b8-183f3b04e271",
   "metadata": {},
   "source": [
    "# Load Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215fe89-40e0-4cdd-a2c1-c878a5f38890",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9c9f98-b2bd-4f36-8c58-5694029254ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'species_embed_100k_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mspecies_embed_100k_path\u001b[49m\n\u001b[1;32m      2\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH)\n\u001b[1;32m      3\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'species_embed_100k_path' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = species_embed_100k_path\n",
    "state_dict = torch.load(MODEL_PATH)\n",
    "state_dict = state_dict[\"state\"][\"model\"]\n",
    "state_dict_without_prefix = {k.replace('model.', ''): v for k, v in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd334a4-3bda-4982-a8e7-d2e62b550edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ef43d6-01fc-4a1f-8b06-410bd37ff066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import FlashFFTConv!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000ba\n",
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gagneurlab/SpeciesLM\", revision = \"downstream_species_lm\")\n",
    "from load_model import load_model\n",
    "\n",
    "monarch_100k_path = \"/data/nasif12/home_if12/huan/monarch/dnam2/bert/slurm/composer/local-bert-checkpoints/lrcorrect__monarch-mixer-pretrain-786dim-80m-parameters/ep19-ba100000-rank0.pt\"\n",
    "monarch_200k_path = \"/data/nasif12/home_if12/huan/monarch/dnam2/bert/slurm/composer/local-bert-checkpoints/lrcorrect__monarch-mixer-pretrain-786dim-80m-parameters/ep39-ba200000-rank0.pt\"\n",
    "agnostic_monarch_path = \"/data/nasif12/home_if12/huan/monarch/dnam2/bert/slurm/composer/local-bert-checkpoints/nonspecies_monarch-mixer-pretrain-786dim-80m-parameters/ep19-ba100000-rank0.pt\"\n",
    "species_embed_100k_path = \"/data/nasif12/home_if12/huan/monarch/dnam2/bert/slurm/species_embedding/local-bert-checkpoints/embed_full_dna_monarch-mixer-pretrain-786dim-80m-parameters2/ep19-ba100000-rank0.pt\"\n",
    "species_embed_yaml = \"/data/nasif12/home_if12/huan/monarch/dnam2/bert/yamls/pretrain/embed_full_dna_monarch-mixer-pretrain-786dim-80m-parameters.yaml\"\n",
    "\n",
    "model = load_model(species_embed_100k_path, species_embed_yaml)\n",
    "\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"gagneurlab/SpeciesLM\", revision=\"downstream_species_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9681f868-65d1-4d30-995b-83c8d5503c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "#model.to(torch.bfloat16).to(device)\n",
    "#model.to(torch.float16).to(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8126f0-c09b-4a07-a2d1-e9c6cc83fba7",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577a4cc7-4154-421c-be0f-b041550de9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (199371223.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    proxy_species =#\"candida_glabrata\"#'saccharomyces_cerevisiae'#'saccharomyces_cerevisiae'#\"candida_glabrata\"\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "proxy_species =#\"candida_glabrata\"#'saccharomyces_cerevisiae'#'saccharomyces_cerevisiae'#\"candida_glabrata\"\n",
    "dataset = (pd.read_csv(\"/s/project/semi_supervised_multispecies/Downstream/MPRA/segal_2015.tsv\", sep=\"\\t\")\n",
    "           .dropna()\n",
    "           .reset_index(drop=True)\n",
    "           .reset_index()\n",
    "           .rename(columns={\"Oligo Sequence\":\"UTR3_seq\"}))\n",
    "groupby_cols = [\"index\", \"UTR3_seq\"]\n",
    "add_stop = True\n",
    "if add_stop:\n",
    "    dataset[\"UTR3_seq\"] = \"TAA\" + dataset[\"UTR3_seq\"]\n",
    "    stop_suffix = \"withstop\"\n",
    "seq_col = \"UTR3_seq\"\n",
    "dataset[seq_col] = dataset[seq_col].str[:300] # truncate longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e3616aa-6335-417d-a9ff-154a2f4c3d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(seq_df_path)\n",
    "# For reading the pombe dataset\n",
    "#dataset = pd.read_csv(seq_df_path, sep=\"\\t\")\n",
    "dataset[seq_col] = dataset[seq_col].str[:300] # truncate longer sequences\n",
    "dataset = dataset.loc[dataset[seq_col].str.len() == 300] # throw out too short sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63822cc7-5c93-45db-bc65-9ef4fc67e224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eae58ddb1ee404baf39f08f8f1c1603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/6594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# delete the species token for the agnostic model\n",
    "tok_func = lambda x: tok_func_embed(x, proxy_species,seq_col)\n",
    "# tok_func = lambda x: tok_func_standard(x,seq_col)\n",
    "\n",
    "ds = Dataset.from_pandas(dataset[[seq_col]])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns(seq_col)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b8279d5-3636-442d-ae35-a57683d877d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m rem_tok_ds[\u001b[38;5;241m5000\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "for k, v in rem_tok_ds[5000].items():\n",
    "    print(f'{k}, {len(v)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "676f80a5-da35-43bf-9698-9cf3b17c4930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 298])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ae42c-e2ff-406e-956f-6dc2124d631f",
   "metadata": {},
   "source": [
    "# Reconstruction Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd306845-4a1d-435b-b5f6-1fd63121e1ce",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5e6d638-8627-4a86-87ac-b24007086a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## printing a matrix\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def print_matrix(data):\n",
    "    display(HTML(\n",
    "       '<table><tr>{}</tr></table>'.format(\n",
    "           '</tr><tr>'.join(\n",
    "               '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "           )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b78f821-b12f-4cb1-b931-93b381b316b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_batch_generator(tokenized_data, dataset, seq_idx, \n",
    "                               special_token_offset, \n",
    "                               kmer_size = kmer_size,\n",
    "                               seq_col = seq_col,\n",
    "                               pred_batch_size = pred_batch_size):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx][seq_col]\n",
    "    label_len = len(label)\n",
    "    if label_len < kmer_size:\n",
    "        print(\"This should not occur\")\n",
    "        return torch.zeros(label_len,label_len,5)\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[2+special_token_offset:label_len-(kmer_size-1)-3+special_token_offset]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        res[masked_indices] = 4\n",
    "        yield res.shape[0] # provide the total size\n",
    "        for batch_idx in range(math.ceil(res.shape[0]/pred_batch_size)):\n",
    "            res_batch = res[batch_idx*pred_batch_size:(batch_idx+1)*pred_batch_size]\n",
    "            res_batch = res_batch.to(device)\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                computation = model(res_batch)\n",
    "                logits = computation[\"logits\"].detach()\n",
    "                #if \"logits\" in computation:\n",
    "                #    logits = computation[\"logits\"].detach()\n",
    "                #else:\n",
    "                #    logits = computation[\"prediction_logits\"].float().detach()\n",
    "                fin_calculation = logits\n",
    "            yield fin_calculation, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b32b2b5-60b4-43cc-bb01-9ee4cf9ae31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a convolutional filter for each nt\n",
    "# the way this works:\n",
    "# The kmer ACGTGC\n",
    "# maps to token 739\n",
    "# the last nt is C\n",
    "# this would be the prediction for the masked nucleotide\n",
    "# from this kmer, if the kmer is the first in masked span\n",
    "# so the first row of column 739 searches for C\n",
    "# in other words filter_ijk = 1 for i = 0, j = 739, k = 2\n",
    "vocab = tokenizer.get_vocab()\n",
    "kmer_list = [\"\".join(x) for x in itertools.product(\"ACGT\",repeat=6)]\n",
    "nt_mapping = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "prb_filter = np.zeros((kmer_size, 4**kmer_size, 4))\n",
    "for kmer in kmer_list:\n",
    "    token = vocab[kmer] - 5 # there are 5 special tokens\n",
    "    for idx, nt in enumerate(kmer):\n",
    "        nt_idx = nt_mapping[nt]\n",
    "        prb_filter[5-idx, token, nt_idx] = 1\n",
    "prb_filter = torch.from_numpy(prb_filter)\n",
    "prb_filter = prb_filter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd36a5a8-660e-4b22-8fae-561d8301c8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(kmer_prediction, \n",
    "                           label_pos,\n",
    "                           max_pos,\n",
    "                           prb_filter=prb_filter,\n",
    "                           kmer_size=kmer_size):   \n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    nt_preds = kmer_prediction[label_pos:(label_pos+kmer_size),:] # extract the right kmers\n",
    "    nt_preds = nt_preds.unsqueeze(2).expand((nt_preds.shape[0],nt_preds.shape[1],4)) # repeat along nt dimension\n",
    "    nt_preds = (nt_preds*prb_filter).sum(axis=1) # filter and add over tokens\n",
    "    nt_preds = nt_preds.sum(axis=0)\n",
    "    nt_prbs = nt_preds/nt_preds.sum() # renormalize\n",
    "    return nt_prbs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb38aa-bae4-48c0-8d30-d60a2d0e8a72",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a0e917-d309-49dc-936f-e95b90aa3ea9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must specify species_id!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m prediction_generator \u001b[38;5;241m=\u001b[39m predict_on_batch_generator(tokenized_data, dataset, no_of_index, special_token_offset \u001b[38;5;241m=\u001b[39m left_special_tokens)\n\u001b[1;32m     31\u001b[0m max_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(prediction_generator)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m predictions, res \u001b[38;5;129;01min\u001b[39;00m prediction_generator:\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# prepare predictions for processing\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     logits \u001b[38;5;241m=\u001b[39m predictions[:,:,\u001b[38;5;241m5\u001b[39m:(\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m+\u001b[39mprb_filter\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;66;03m# remove any non k-mer dims\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     kmer_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mpredict_on_batch_generator\u001b[0;34m(tokenized_data, dataset, seq_idx, special_token_offset, kmer_size, seq_col, pred_batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m res_batch \u001b[38;5;241m=\u001b[39m res_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     computation \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m computation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#if \"logits\" in computation:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#    logits = computation[\"logits\"].detach()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#    logits = computation[\"prediction_logits\"].float().detach()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/src/bert_layers.py:986\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, species_id, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    983\u001b[0m     masked_tokens_mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    985\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 986\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecies_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecies_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_encoded_layers\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_all_encoded_layers:\n\u001b[1;32m   1004\u001b[0m     prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/src/bert_layers.py:797\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, species_id, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_ids)\n\u001b[0;32m--> 797\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecies_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecies_id\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m position_encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m subset_mask \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/src/bert_layers.py:118\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, species_id, past_key_values_length, return_position_encodings)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m species_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_species_embeddings:\n\u001b[0;32m--> 118\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMust specify species_id!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Setting the token_type_ids to the registered buffer in constructor\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# where it is all zeros, which usually occurs when it's auto-generated;\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# registered buffer helps users when tracing the model without passing\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# token_type_ids, solves issue #5664\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Must specify species_id!"
     ]
    }
   ],
   "source": [
    "predicted_prbs,gts = [],[]\n",
    "prev_len = 0\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    print(no_of_index)\n",
    "    #if no_of_index > 10:\n",
    "    #    break\n",
    "    label = dataset.iloc[no_of_index][seq_col]\n",
    "    label_len = len(label)\n",
    "    \n",
    "    left_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"left\")\n",
    "    right_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"right\")\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 11: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25]))\n",
    "            gts.append(label[i])\n",
    "        added_len = len(predicted_prbs) - prev_len\n",
    "        prev_len = len(predicted_prbs)\n",
    "        assert added_len == len(label)\n",
    "        continue\n",
    "\n",
    "    # we do a batched predict to process the sequence\n",
    "    batch_start = 0\n",
    "    pos = 0\n",
    "    prediction_generator = predict_on_batch_generator(tokenized_data, dataset, no_of_index, special_token_offset = left_special_tokens)\n",
    "    max_idx = next(prediction_generator)\n",
    "    for predictions, res in prediction_generator:\n",
    "    \n",
    "        # prepare predictions for processing\n",
    "        logits = predictions[:,:,5:(5+prb_filter.shape[1])] # remove any non k-mer dims\n",
    "        kmer_preds = torch.softmax(logits,dim=2)\n",
    "        # remove special tokens:\n",
    "        kmer_preds = kmer_preds[:,(left_special_tokens):(kmer_preds.shape[1] - right_special_tokens),:]\n",
    "        max_pos = kmer_preds.shape[1] - 1\n",
    "        # pad to predict first 5 and last 5 nt\n",
    "        padded_tensor = torch.zeros((kmer_preds.shape[0],2*(kmer_size-1) + kmer_preds.shape[1],kmer_preds.shape[2]),device=device)\n",
    "        padded_tensor[:,kmer_size-1:-(kmer_size-1),:] = kmer_preds\n",
    "        kmer_preds = padded_tensor\n",
    "        \n",
    "        while pos < label_len:\n",
    "            # get prediction\n",
    "            theoretical_idx = min(max(pos-5,0),max_idx-1) # idx if we did it all in one batch\n",
    "            actual_idx = max(theoretical_idx - batch_start,0) \n",
    "            if actual_idx >= kmer_preds.shape[0]:\n",
    "                break\n",
    "            kmer_prediction = kmer_preds[actual_idx]\n",
    "            nt_prbs = extract_prbs_from_pred(kmer_prediction=kmer_prediction, \n",
    "                                             label_pos=pos,\n",
    "                                             max_pos=max_pos)\n",
    "            predicted_prbs.append(nt_prbs)\n",
    "            #print(nt_prbs.shape)\n",
    "            #print(label_len)\n",
    "            # extract ground truth\n",
    "            gt = label[pos]\n",
    "            gts.append(gt)\n",
    "            # update\n",
    "            pos += 1\n",
    "        \n",
    "        batch_start = pos - 5\n",
    "\n",
    "    added_len = len(predicted_prbs) - prev_len\n",
    "    prev_len = len(predicted_prbs)\n",
    "    assert added_len == len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d274b04-7438-4a79-830a-426f9cfa8b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prbs_arr = np.stack(predicted_prbs).reshape((no_of_index + 1, 300, 4))\n",
    "prbs_tensor = torch.Tensor(np.stack(predicted_prbs))\n",
    "torch.save(prbs_tensor, 'pred.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83af678c-f34a-4ba8-b42f-d4ca9d36531f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1978200, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(predicted_prbs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788438d-ce40-476f-8bdc-ed77bad0bfe5",
   "metadata": {},
   "source": [
    "# Embedding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf37faa-0ddf-4cbb-9507-19005b96dc84",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_on_batch(tokenized_data, dataset, seq_idx, \n",
    "                   special_token_offset,\n",
    "                   target_layer = target_layer):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx][seq_col]\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        print(\"This should not occur\")\n",
    "        return torch.zeros(label_len,label_len,768)\n",
    "    else:\n",
    "        res = tokenized_data['input_ids'].clone()\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            #embedding = model(res, output_all_encoded_layers=True)['hidden_states']\n",
    "            embedding = model(res, output_hidden_states=True)['hidden_states']\n",
    "    if isinstance(target_layer, int):    \n",
    "        embedding = embedding[target_layer]\n",
    "    elif len(target_layer) == 1:\n",
    "        embedding = torch.stack(embedding[target_layer[0]:],axis=0)\n",
    "        embedding = torch.mean(embedding, axis=0)\n",
    "    else:\n",
    "        embedding = torch.stack(embedding[target_layer[0]:target_layer[1]],axis=0)\n",
    "        embedding = torch.mean(embedding, axis=0)   \n",
    "    embedding = embedding.detach().cpu().numpy() \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ceb58cb-8cfb-4de8-bd6a-ee5e73537952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embedding_from_pred(hidden_states, batch_pos):   \n",
    "    pred_pos_min = min(max(pos - 5, 0), hidden_states.shape[1]-1)\n",
    "    pred_pos_max = min(max(pos, 0), hidden_states.shape[1]-1)\n",
    "    token_embedding = hidden_states[batch_pos, pred_pos_min:pred_pos_max+1, :]\n",
    "    token_embedding = token_embedding.mean(axis=0)\n",
    "    return token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652363-06c5-4c75-9ca2-d480da793fec",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26683cd0-c57c-4c70-b3a1-7e2d64e0e474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14172it [02:44, 86.16it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "averaged_embeddings = []\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    embeddings = []\n",
    "\n",
    "    label = dataset.iloc[no_of_index][seq_col]\n",
    "    label_len = len(label)\n",
    "    \n",
    "    left_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"left\")\n",
    "    right_special_tokens = count_special_tokens(tokenized_data['input_ids'].numpy()[0], tokenizer, where=\"right\")\n",
    "\n",
    "    if label_len < 11: \n",
    "        averaged_embeddings.append(np.array([0.0]*768))\n",
    "        continue\n",
    "\n",
    "    hidden_states = embed_on_batch(tokenized_data, dataset, no_of_index, special_token_offset = left_special_tokens)\n",
    "    avg = hidden_states.mean(axis=(0,1))\n",
    "    \n",
    "    averaged_embeddings.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbbc000c-e1f2-4bdf-829f-78b92cc95edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = np.stack(averaged_embeddings)\n",
    "#prbs_tensor = torch.Tensor(np.stack(averaged_embeddings))\n",
    "name = \"../embeddings_outputs/\" + \"vanilla_species_candidatoken\" + \"-\" + \"segal\" + \"/embeddings.npy\" \n",
    "np.save(name, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a642e-3b0c-448a-a68d-9f0e7172e4a0",
   "metadata": {},
   "source": [
    "## Comparing Our and their outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbdb6579-e056-468f-889c-d06afd066f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/outputs/embedding-species_downstream_300-static_schizosaccharomyces_pombe-pombe-512_withstop_bertstyle/embeddings.npy\"\n",
    "their_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/outputs/embedding-downstream_300-static_agnostic-pombe-512_withstop_bertstyle/embeddings.npy\"\n",
    "mpra_their_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/outputs/embedding-species_downstream_300-static_candida_glabrata-segal-512_withstop_bertstyle/embeddings.npy\"\n",
    "#their_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/outputs/embedding-species_downstream_300-static_candida_glabrata-scer_downstream_fixedlen-512_withstop_bertstyle/embeddings.npy\"\n",
    "#their_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/outputs/embedding-species_downstream_300-static_candida_glabrata-segal-512_withstop_bertstyle/embeddings.npy\"\n",
    "#their_path = \"/s/project/semi_supervised_multispecies/Zenodo/zenodo/outputs/embedding-downstream_300-static_agnostic-scer_downstream_fixedlen-512_withstop_bertstyle/embeddings.npy\"\n",
    "their_ds = np.load(mpra_their_path)\n",
    "our_path = \"../embeddings_outputs/vanilla_species_pombetoken-pombe/embeddings.npy\"\n",
    "our_path = \"../embeddings_outputs/vanilla_agnostic-pombe/embeddings.npy\"\n",
    "#our_path = \"../embeddings_outputs/vanilla_species_candidatoken-scer/embeddings.npy\"\n",
    "our_ds = np.load(our_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d0fe82a-2f01-4976-b026-835ad1f26757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14172, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d915744-9853-4404-aa6c-24e533479044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "tensor1 = torch.Tensor(their_ds)\n",
    "tensor2 = torch.Tensor(embeddings)\n",
    "\n",
    "# Normalize the tensors along dimension 1\n",
    "tensor1_norm = F.normalize(tensor1, p=2, dim=1)\n",
    "tensor2_norm = F.normalize(tensor2, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = torch.mm(tensor1_norm, tensor2_norm.transpose(0,1))\n",
    "\n",
    "# Get the indices of the vectors with highest similarity\n",
    "max_sim, max_sim_indices = torch.max(cos_sim, dim=1)\n",
    "\n",
    "# max_sim contains the maximum similarity values for each vector in tensor1\n",
    "# max_sim_indices contains the indices of the most similar vector in tensor2 for each vector in tensor1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4ecb1-b464-4f4e-a90a-a85651081de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_diff = 0\n",
    "for idx, i in enumerate(max_sim_indices):\n",
    "    print(max_sim[idx], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a968a73-84d4-4a47-88bb-76d7504774dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 tensor(0.9951) tensor(4291)\n",
      "831 tensor(0.9866) tensor(1893)\n",
      "1852 tensor(0.9550) tensor(1833)\n",
      "2180 tensor(0.9966) tensor(4291)\n",
      "4084 tensor(0.9986) tensor(4291)\n",
      "6555 tensor(0.9986) tensor(4291)\n"
     ]
    }
   ],
   "source": [
    "diffs = [478, 832, 1853, 2181, 4085, 6556]\n",
    "\n",
    "for i in diffs:\n",
    "    idx = i - 1\n",
    "    print(idx, max_sim[idx], max_sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2-mixer",
   "language": "python",
   "name": "m2-mixer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
